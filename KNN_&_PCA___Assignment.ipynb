{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both**\n",
        "**classification and regression problems?**\n",
        "\n",
        "Ans:- K-Nearest Neighbors (KNN) is a non-parametric supervised learning algorithm used for both classification and regression. As a \"lazy learner,\" it doesn't build a model during training; instead, it simply stores the entire dataset. When a new data point needs a prediction, it finds the 'k' closest data points (its neighbors) from the training data and uses their values to make a decision. The 'k' is a user-defined positive integer.\n",
        "\n",
        "---\n",
        "\n",
        "**How KNN Works**\n",
        "\n",
        "The core of the KNN algorithm lies in these steps:\n",
        "\n",
        "1. Choose a value for k: This is the number of neighbors to consider.\n",
        "\n",
        "2. Calculate distance: The algorithm measures the distance between the new data point and every other point in the training dataset. The most common distance metric is the Euclidean distance.\n",
        "\n",
        "\n",
        "3. Identify neighbors: It selects the k data points with the smallest distances.\n",
        "\n",
        "4. Make a prediction: The method of prediction differs for classification and regression tasks.\n",
        "\n",
        "---\n",
        "\n",
        "**KNN for Classification**\n",
        "\n",
        "In a classification problem, the goal is to predict a categorical label (e.g., \"spam\" or \"not spam\"). For a new data point, KNN works like a majority vote. After identifying the k nearest neighbors, the algorithm assigns the new point to the class that is most frequent among those neighbors. For example, if k=5 and three of the five nearest neighbors belong to \"Class A\" and two belong to \"Class B\", the new data point will be classified as \"Class A.\"\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**KNN for Regression**\n",
        "\n",
        "In a regression problem, the goal is to predict a continuous numerical value (e.g., a house price). Instead of a majority vote, KNN calculates the average of the values of the k nearest neighbors. If the k=3 nearest neighbors of a new data point have values of 2.5, 3.2, and 2.8, the predicted value for the new point would be their average: (2.5+3.2+2.8)/3=2.83.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PmXgigquFGKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the Curse of Dimensionality and how does it affect KNN**\n",
        "**performance?**\n",
        "\n",
        "Ans:-  The Curse of Dimensionality refers to various phenomena that arise when analyzing or modeling data in high-dimensional spaces, where the number of features becomes very large. üöÄ Essentially, as you add more and more dimensions (features) to a dataset, the space becomes exponentially larger, making the data points incredibly sparse. üåå Imagine spreading a handful of marbles on a line (1D), then in a square (2D), and finally in a huge cube (3D) and beyond. In high dimensions, all the data points are far away from each other, which breaks down the fundamental assumptions of many algorithms.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**How it Curses KNN Performance üìâ**\n",
        "\n",
        "KNN is particularly susceptible to the Curse of Dimensionality because it relies on the concept of distance and \"neighborhood.\" When you have a high number of dimensions, these are the main ways KNN's performance suffers:\n",
        "\n",
        "- Distance Loses Meaning: üìè In high-dimensional spaces, the distance between any two data points tends to become almost the same. This phenomenon is called \"distance concentration.\" Since KNN's logic is based on finding the closest neighbors, if all points are roughly equidistant, the notion of a \"nearest\" neighbor becomes meaningless. The algorithm can't reliably determine which points are truly similar.\n",
        "\n",
        "\n",
        "- Data Sparsity: üßë‚Äçü§ù‚Äçüßë With the data points spread so far apart, a new data point might not have any truly \"local\" neighbors. The algorithm will be forced to consider neighbors that are actually very far away, and these distant points are unlikely to be similar to the new point, leading to poor predictions. This also increases the risk of overfitting, as the model may pick up on noise from these distant, irrelevant points.\n",
        "\n",
        "\n",
        "\n",
        "- Increased Computational Cost: ‚è±Ô∏è Calculating the distance between a new point and every single point in the training dataset is the most computationally expensive part of KNN. As the number of dimensions increases, this calculation becomes significantly more complex and time-consuming, making the algorithm slow and inefficient.\n",
        "\n",
        "In short, the Curse of Dimensionality makes KNN's core assumption‚Äîthat nearby points are similar‚Äîfail, leading to a breakdown in its predictive power and efficiency. üí•\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BRCbix0aGU4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is Principal Component Analysis (PCA)? How is it different** **from**\n",
        "**feature selection?**\n",
        "\n",
        "Ans:- Principal Component Analysis (PCA) is a powerful, unsupervised dimensionality reduction technique. üìä Its main goal is to transform a high-dimensional dataset into a new set of dimensions, called principal components (PCs), that are uncorrelated and ordered by the amount of variance they explain.\n",
        "\n",
        "\n",
        "Think of it like this: if you have a cloud of data points in a 3D space, PCA finds the best 2D plane to project those points onto, so that you lose the least amount of information. The first principal component (PC1) is the direction where the data has the most variance, and each subsequent component is orthogonal (perpendicular) to the previous one while capturing the next highest amount of variance. The original features are combined to create these new components. By keeping only the first few principal components, you can represent a complex dataset in a much simpler form. üßê\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**PCA vs. Feature Selection: What's the Difference?**\n",
        "\n",
        "While both PCA and feature selection are used for dimensionality reduction, they achieve this goal in fundamentally different ways.\n",
        "\n",
        "\n",
        "Principal Component Analysis **(PCA)**\tFeature Selection\n",
        "Method\tFeature Extraction üõ†Ô∏è It creates new, artificial features (principal components) that are linear combinations of the original features. The original features are not preserved in their initial form.\tFeature Subsetting ‚úÇÔ∏è It selects a subset of the original features and discards the rest. The original features are retained, just a smaller number of them.\n",
        "Output\tThe output is a new set of principal components. These components are uncorrelated with each other.\tThe output is a subset of the original features. These features may still be correlated.\n",
        "Information\tIt aims to retain the maximum amount of variance (information) from the original dataset by finding the most informative directions. Some information loss is inevitable, but it‚Äôs minimized.\tIt removes less important features entirely, which can lead to a complete loss of information associated with those specific features.\n",
        "Interpretability\tThe new principal components are often difficult to interpret because they are a combination of the original features. For example, \"PC1\" doesn't have a clear, real-world meaning.\tThe selected features are the original ones, so they retain their original meaning and are easy to interpret. For example, \"age\" and \"income\" are still \"age\" and \"income.\"\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "In summary, PCA is a feature extraction technique that creates new, more compact features, while feature selection is a feature subsetting technique that chooses from the existing ones. The choice between them depends on whether you need interpretability or are more concerned with minimizing information loss and improving model performance.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ls6dYJHjHK9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What are eigenvalues and eigenvectors in PCA, and why are they**\n",
        "**important?**\n",
        "\n",
        "\n",
        "Ans:- In Principal Component Analysis (PCA), eigenvalues and eigenvectors are fundamental concepts from linear algebra that reveal the structure and key directions within a dataset. They are the \"engine\" behind the algorithm, identifying and ordering the new dimensions (principal components) that PCA creates. ‚öôÔ∏è\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "In Principal Component Analysis (PCA), eigenvalues and eigenvectors are fundamental concepts from linear algebra that reveal the structure and key directions within a dataset. They are the \"engine\" behind the algorithm, identifying and ordering the new dimensions (principal components) that PCA creates. ‚öôÔ∏è\n",
        "\n",
        "\n",
        "**What are Eigenvalues and Eigenvectors?**\n",
        "\n",
        "- Eigenvectors are special vectors that, when a linear transformation (like a matrix multiplication) is applied, only get scaled‚Äîthey don't change their direction. In the context of PCA, the eigenvectors of the covariance matrix are the new axes or principal components.  These eigenvectors are orthogonal (perpendicular) to each other, and they represent the directions of maximum variance in the data. The first eigenvector is the direction where the data is most spread out, the second is the next most spread out direction (perpendicular to the first), and so on.\n",
        "\n",
        "\n",
        "\n",
        "- Eigenvalues are the scalar values corresponding to each eigenvector. An eigenvalue tells you the magnitude of the variance in the direction of its corresponding eigenvector. A large eigenvalue means that the variance (spread) along that eigenvector's direction is high, indicating that it captures a lot of the data's information. A small eigenvalue means the variance is low, suggesting that direction is less important for describing the data.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Why are They Important in PCA?**\n",
        "\n",
        " Eigenvalues and eigenvectors are crucial because they directly enable the dimensionality reduction process in PCA. Here's how:\n",
        "\n",
        "1. Finding the Principal Components: PCA computes the covariance matrix of the dataset, which summarizes the relationships between features. The eigenvectors of this covariance matrix become the principal components. They literally define the new axes of your transformed data space. üó∫Ô∏è\n",
        "\n",
        "\n",
        "\n",
        "2. Ranking the Components: PCA sorts the eigenvalues in descending order. This ranking allows the algorithm to prioritize the principal components based on the amount of variance they explain. The eigenvector with the largest eigenvalue is the first principal component (PC1), the one with the second-largest is PC2, and so on.\n",
        "\n",
        "\n",
        "\n",
        "3. Dimensionality Reduction: By sorting the principal components by their corresponding eigenvalues, you can choose to keep only the top k components. Since these components capture the most variance, you can discard the ones with small eigenvalues, effectively reducing the number of dimensions while preserving the most important information from the original dataset. For instance, if the first two principal components explain 95% of the total variance, you can reduce a 10-dimensional dataset to just 2 dimensions with minimal information loss. üíæ‚û°Ô∏èüöÄ\n",
        "\n",
        "\n",
        "In short, eigenvectors give PCA the directions to project the data, and eigenvalues give the importance of those directions, allowing for a principled way to reduce dimensionality.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bhETa56RIIbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: How do KNN and PCA complement each other when applied in a single**\n",
        "**pipeline?**\n",
        "\n",
        "Ans:-  PCA and K-Nearest Neighbors (KNN) form a powerful machine learning duo. ü¶∏‚Äç‚ôÇÔ∏è In essence, PCA acts as a pre-processing step that perfectly prepares the data for KNN, addressing its primary weakness: the Curse of Dimensionality.\n",
        "\n",
        "\n",
        "---\n",
        "**The Problem: KNN in High Dimensions**\n",
        "\n",
        "As we've discussed, KNN struggles when faced with a high number of features (dimensions).\n",
        "\n",
        "- The Curse of Dimensionality üìâ makes the distance between points less meaningful.\n",
        "\n",
        "- Computational cost skyrockets as KNN has to calculate distances in a huge space. üê¢\n",
        "\n",
        "\n",
        "---\n",
        "**The Solution: PCA to the Rescue! ‚ú®**\n",
        "\n",
        "This is where PCA steps in as the perfect partner. By applying PCA before KNN, you can transform the high-dimensional data into a low-dimensional space while preserving the most important information.\n",
        "\n",
        "This combination works so well because:\n",
        "\n",
        "- **PCA creates a more efficient feature space.** It takes your many, possibly correlated, features and transforms them into a few, uncorrelated principal components. These new components are ordered by the amount of variance they explain.\n",
        "\n",
        "- **It makes distance meaningful again.** By projecting data onto the principal components, the \"neighborhood\" concept is restored. Points that are close in this new, compact space are more likely to be truly similar.\n",
        "\n",
        "- **It makes KNN much faster. üöÄ** With fewer dimensions to work with, KNN can calculate distances and find neighbors much more quickly.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**The Combined Pipeline in Action ‚öôÔ∏è**\n",
        "\n",
        "Here‚Äôs how they work together in a typical machine learning pipeline:\n",
        "\n",
        "***High-Dimensional Data üìä ‚û°Ô∏è PCA Transformation üå™Ô∏è ‚û°Ô∏è Low-Dimensional Data ‚ú® ‚û°Ô∏è KNN Algorithm ü§ñ ‚û°Ô∏è Final Prediction üéØ***\n",
        "\n",
        "**1. Apply PCA:** You first train a PCA model on your high-dimensional training data to find the principal components.\n",
        "\n",
        "**2. Transform the Data**: You use the trained PCA model to transform your dataset into a new, low-dimensional version.\n",
        "\n",
        "**3. Apply KNN:** You then use this new, compact dataset with the KNN algorithm to make predictions. For any new data point, you must first transform it using the same PCA model before feeding it to KNN.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        " **Key Benefits of Combining PCA and KNN**\n",
        "Improved Accuracy ‚úÖ: By filtering out the noise and focusing on the directions of maximum variance, PCA can help KNN make more robust and accurate predictions.\n",
        "\n",
        "**Massively Faster Performance ‚ö°:** Fewer dimensions mean far fewer distance calculations, making the entire process significantly faster and more scalable.\n",
        "\n",
        "**Overcomes the Curse of Dimensionality üåå:** This is the primary benefit. PCA effectively sidesteps the issues of data sparsity and meaningless distances in high dimensions.\n",
        "\n",
        "\n",
        "In short, PCA takes a messy, high-dimensional dataset and \"cleans it up\" by creating a compact and informative representation, allowing KNN to do its job effectively and efficiently. It's a classic example of using one algorithm to overcome the limitations of another. **üí°**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fUfOiFM8JJ2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset:**\n",
        "\n",
        "**Use the Wine Dataset from sklearn.datasets.load_wine().**\n",
        "\n",
        "**Question 6: Train a KNN Classifier on the Wine dataset with and without feature**\n",
        "**scaling. Compare model accuracy in both cases.**\n",
        "\n",
        "Ans:-  **Comparison of KNN Accuracy With and Without Feature Scaling**\n",
        "\n",
        "The K-Nearest Neighbors (KNN) algorithm is a distance-based classifier, which makes it highly sensitive to the magnitude and scale of the features. Without proper scaling, features with a larger range of values can dominate the distance calculation, leading to a biased and less accurate model.\n",
        "\n",
        "By applying a StandardScaler to the data, we normalize the features to have a mean of 0 and a standard deviation of 1. This ensures that all features contribute equally to the distance metric, allowing KNN to find the true nearest neighbors more effectively.\n",
        "\n",
        "As the code and output below demonstrate, training a KNN classifier on the Wine dataset with feature scaling results in a significant improvement in accuracy compared to training on the unscaled data.\n",
        "\n",
        "**Python Code and Output**"
      ],
      "metadata": {
        "id": "f07rJil3LXWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine_data = load_wine()\n",
        "X = wine_data.data\n",
        "y = wine_data.target\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"--------------------------------------------------\")\n",
        "print(\"KNN Classifier without Feature Scaling\")\n",
        "print(\"--------------------------------------------------\")\n",
        "\n",
        "# 3. Train KNN without scaling\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "print(f\"Accuracy without scaling: {accuracy_unscaled:.4f}\\n\")\n",
        "\n",
        "print(\"--------------------------------------------------\")\n",
        "print(\"KNN Classifier with Feature Scaling\")\n",
        "print(\"--------------------------------------------------\")\n",
        "\n",
        "# 4. Apply StandardScaler to the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 5. Train KNN with scaled data\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy with scaling: {accuracy_scaled:.4f}\\n\")\n",
        "\n",
        "# 6. Conclusion\n",
        "print(\"--------------------------------------------------\")\n",
        "print(\"Conclusion\")\n",
        "print(\"--------------------------------------------------\")\n",
        "print(f\"The accuracy improved from {accuracy_unscaled:.2%} to {accuracy_scaled:.2%} after applying feature scaling.\")\n",
        "print(\"This demonstrates the critical importance of scaling features for distance-based algorithms like KNN.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwGmEYPNMTd9",
        "outputId": "bc8d6208-bde5-4735-f8d9-f16e6950af30"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "KNN Classifier without Feature Scaling\n",
            "--------------------------------------------------\n",
            "Accuracy without scaling: 0.7407\n",
            "\n",
            "--------------------------------------------------\n",
            "KNN Classifier with Feature Scaling\n",
            "--------------------------------------------------\n",
            "Accuracy with scaling: 0.9630\n",
            "\n",
            "--------------------------------------------------\n",
            "Conclusion\n",
            "--------------------------------------------------\n",
            "The accuracy improved from 74.07% to 96.30% after applying feature scaling.\n",
            "This demonstrates the critical importance of scaling features for distance-based algorithms like KNN.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Train a PCA model on the Wine dataset and print the explained **variance**\n",
        "**ratio of each principal component.**\n",
        "\n",
        "Ans:- **Explained Variance Ratio of Principal Components**\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms a dataset into a new set of dimensions, called principal components, which are ordered by the amount of variance they explain. The explained variance ratio for each component indicates the proportion of the dataset's total variance that is captured by that component.\n",
        "\n",
        "The code below first standardizes the Wine dataset, a crucial step for PCA, and then fits a PCA model to it. The output shows the explained variance ratio for each of the 13 principal components.\n",
        "\n",
        "As you can see from the output, the first principal component (PC1) alone captures a significant portion of the total variance, and the first three components together account for a substantial majority of the dataset's information. This demonstrates that a significant amount of the data's variance can be represented in a much lower-dimensional space.\n",
        "\n",
        "**Python Code and Output**\n",
        "\n"
      ],
      "metadata": {
        "id": "bXZvm3EeMjDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine_data = load_wine()\n",
        "X = wine_data.data\n",
        "feature_names = wine_data.feature_names\n",
        "\n",
        "# 2. Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Train a PCA model\n",
        "# We set n_components=None to get all principal components\n",
        "pca = PCA(n_components=None)\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# 4. Get the explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# 5. Create a DataFrame for a clear display of the results\n",
        "results_df = pd.DataFrame({\n",
        "    'Principal Component': [f'PC{i+1}' for i in range(len(explained_variance_ratio))],\n",
        "    'Explained Variance Ratio': explained_variance_ratio,\n",
        "    'Cumulative Explained Variance': cumulative_variance\n",
        "})\n",
        "\n",
        "# Print the results\n",
        "print(\"--------------------------------------------------\")\n",
        "print(\"PCA Explained Variance Ratio for the Wine Dataset\")\n",
        "print(\"--------------------------------------------------\")\n",
        "print(results_df)\n",
        "print(\"\\n\")\n",
        "print(\"--------------------------------------------------\")\n",
        "print(\"Summary of Explained Variance\")\n",
        "print(\"--------------------------------------------------\")\n",
        "print(f\"The first principal component (PC1) explains {explained_variance_ratio[0]:.2%} of the total variance.\")\n",
        "print(f\"The first two principal components (PC1 & PC2) together explain {cumulative_variance[1]:.2%} of the total variance.\")\n",
        "print(f\"The first three principal components (PC1, PC2, & PC3) together explain {cumulative_variance[2]:.2%} of the total variance.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxjLj5wuNPRd",
        "outputId": "51e14535-3bf2-45fa-8a50-994d17c851f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "PCA Explained Variance Ratio for the Wine Dataset\n",
            "--------------------------------------------------\n",
            "   Principal Component  Explained Variance Ratio  \\\n",
            "0                  PC1                  0.361988   \n",
            "1                  PC2                  0.192075   \n",
            "2                  PC3                  0.111236   \n",
            "3                  PC4                  0.070690   \n",
            "4                  PC5                  0.065633   \n",
            "5                  PC6                  0.049358   \n",
            "6                  PC7                  0.042387   \n",
            "7                  PC8                  0.026807   \n",
            "8                  PC9                  0.022222   \n",
            "9                 PC10                  0.019300   \n",
            "10                PC11                  0.017368   \n",
            "11                PC12                  0.012982   \n",
            "12                PC13                  0.007952   \n",
            "\n",
            "    Cumulative Explained Variance  \n",
            "0                        0.361988  \n",
            "1                        0.554063  \n",
            "2                        0.665300  \n",
            "3                        0.735990  \n",
            "4                        0.801623  \n",
            "5                        0.850981  \n",
            "6                        0.893368  \n",
            "7                        0.920175  \n",
            "8                        0.942397  \n",
            "9                        0.961697  \n",
            "10                       0.979066  \n",
            "11                       0.992048  \n",
            "12                       1.000000  \n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Summary of Explained Variance\n",
            "--------------------------------------------------\n",
            "The first principal component (PC1) explains 36.20% of the total variance.\n",
            "The first two principal components (PC1 & PC2) together explain 55.41% of the total variance.\n",
            "The first three principal components (PC1, PC2, & PC3) together explain 66.53% of the total variance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2**\n",
        "**components). Compare the accuracy with the original dataset.**\n",
        "\n",
        "Ans:-  **Training KNN on a PCA-Transformed Dataset**\n",
        "\n",
        "This exercise demonstrates the synergy between Principal Component Analysis (PCA) and the K-Nearest Neighbors (KNN) algorithm. By using PCA as a pre-processing step, we can reduce the dimensionality of the dataset from 13 original features to just 2 principal components, while retaining a significant amount of the data's variance.\n",
        "\n",
        "**The code below performs the following steps:**\n",
        "\n",
        "- Establishes a Baseline: First, a KNN classifier is trained on the full, scaled dataset to provide a performance benchmark.\n",
        "\n",
        "- Applies PCA: A PCA model is then trained to transform the data, keeping only the top 2 principal components.\n",
        "\n",
        "- Trains a Second KNN Model: A new KNN classifier is trained on this reduced, 2-dimensional dataset.\n",
        "\n",
        "- Compares the Results: The accuracy of both models is compared.\n",
        "\n",
        "The results show that even after a substantial reduction in dimensionality, the KNN model trained on the PCA-transformed data not only maintains a high level of accuracy but actually achieves a slightly higher score. This highlights PCA's ability to simplify a dataset by focusing on the most informative features, which can sometimes lead to improved model performance by filtering out noise.\n",
        "\n",
        "**Python Code and Output**\n",
        "\n"
      ],
      "metadata": {
        "id": "6Msv-UWBNWjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset and split the data\n",
        "wine_data = load_wine()\n",
        "X = wine_data.data\n",
        "y = wine_data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Establish a baseline: KNN on scaled data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"--------------------------------------------------\")\n",
        "print(\"Accuracy of KNN on Scaled Data (Original Features)\")\n",
        "print(\"--------------------------------------------------\")\n",
        "print(f\"Accuracy: {accuracy_scaled:.4f}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# 3. Apply PCA and train KNN on the transformed data\n",
        "# Train PCA to retain the top 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Train a new KNN classifier on the PCA-transformed data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(\"--------------------------------------------------\")\n",
        "print(\"Accuracy of KNN on PCA-Transformed Data (Top 2 Components)\")\n",
        "print(\"--------------------------------------------------\")\n",
        "print(f\"Accuracy: {accuracy_pca:.4f}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# 4. Compare the accuracies\n",
        "print(\"--------------------------------------------------\")\n",
        "print(\"Comparison of Accuracies\")\n",
        "print(\"--------------------------------------------------\")\n",
        "print(f\"Accuracy on original scaled data:      {accuracy_scaled:.4f}\")\n",
        "print(f\"Accuracy on PCA-transformed data (2D): {accuracy_pca:.4f}\")\n",
        "print(\"This shows that even with a significant reduction in dimensionality (from 13 to 2 features),\")\n",
        "print(\"the KNN model's performance remains highly competitive and accurate.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9bNFF6mOQnI",
        "outputId": "7abb2a20-b66b-46b4-d1df-dfa97c8b014c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "Accuracy of KNN on Scaled Data (Original Features)\n",
            "--------------------------------------------------\n",
            "Accuracy: 0.9630\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Accuracy of KNN on PCA-Transformed Data (Top 2 Components)\n",
            "--------------------------------------------------\n",
            "Accuracy: 0.9815\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Comparison of Accuracies\n",
            "--------------------------------------------------\n",
            "Accuracy on original scaled data:      0.9630\n",
            "Accuracy on PCA-transformed data (2D): 0.9815\n",
            "This shows that even with a significant reduction in dimensionality (from 13 to 2 features),\n",
            "the KNN model's performance remains highly competitive and accurate.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Train a KNN Classifier with different distance metrics (euclidean,**\n",
        "**manhattan) on the scaled Wine dataset and compare the results.**\n",
        "\n",
        "\n",
        "Ans:-  **Comparing KNN with Euclidean vs. Manhattan Distance üìèüèôÔ∏è**\n",
        "\n",
        "The choice of distance metric is a critical hyperparameter for the K-Nearest Neighbors (KNN) algorithm. It determines how the \"closeness\" of neighbors is measured, and thus, how a final prediction is made. We will compare two common metrics:\n",
        "\n",
        "Euclidean Distance: The most popular metric, it calculates the straight-line distance between two points in a space. It is sensitive to all feature dimensions equally.\n",
        "\n",
        "Manhattan Distance: Also known as \"city block\" distance, it calculates the distance by summing the absolute differences of the coordinates. It's like navigating a grid.\n",
        "\n",
        "For this comparison, we'll first scale the Wine dataset to ensure fair evaluation, as KNN is sensitive to feature scales regardless of the distance metric. The results show that for this specific dataset, the choice of metric has a minimal impact on the final accuracy.\n",
        "\n",
        "**Python Code and Output**"
      ],
      "metadata": {
        "id": "kEVLDtg4OTEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine_data = load_wine()\n",
        "X = wine_data.data\n",
        "y = wine_data.target\n",
        "\n",
        "# 2. Split and scale the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"--------------------------------------------------\")\n",
        "print(\"KNN Classifier with Different Distance Metrics üéØ\")\n",
        "print(\"--------------------------------------------------\")\n",
        "\n",
        "# 3. Train KNN with Euclidean Distance\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "print(f\"Accuracy with Euclidean Distance (metric='euclidean'): {accuracy_euclidean:.4f}\")\n",
        "\n",
        "# 4. Train KNN with Manhattan Distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "print(f\"Accuracy with Manhattan Distance (metric='manhattan'): {accuracy_manhattan:.4f}\")\n",
        "\n",
        "print(\"\\n--------------------------------------------------\")\n",
        "print(\"Summary of Results\")\n",
        "print(\"--------------------------------------------------\")\n",
        "print(\"For the scaled Wine dataset, both Euclidean and Manhattan distance metrics provide excellent accuracy.\")\n",
        "print(\"The slight difference in performance can depend on the underlying data structure.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3VQQHmTOzsM",
        "outputId": "78cab952-2d24-4817-b15b-d3c47ef27f4a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "KNN Classifier with Different Distance Metrics üéØ\n",
            "--------------------------------------------------\n",
            "Accuracy with Euclidean Distance (metric='euclidean'): 0.9630\n",
            "Accuracy with Manhattan Distance (metric='manhattan'): 0.9630\n",
            "\n",
            "--------------------------------------------------\n",
            "Summary of Results\n",
            "--------------------------------------------------\n",
            "For the scaled Wine dataset, both Euclidean and Manhattan distance metrics provide excellent accuracy.\n",
            "The slight difference in performance can depend on the underlying data structure.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You are working with a high-dimensional gene expression dataset to**\n",
        "**classify patients with different types of cancer.**\n",
        "**Due to the large number of features and a small number of samples, traditional models**\n",
        "**overfit.**\n",
        "\n",
        "**Explain how you would:**\n",
        "\n",
        "**‚óè Use PCA to reduce dimensionality**\n",
        "\n",
        "**‚óè Decide how many components to keep**\n",
        "\n",
        "**‚óè Use KNN for classification post-dimensionality reduction**\n",
        "\n",
        "**‚óè Evaluate the model**\n",
        "\n",
        "**‚óè Justify this pipeline to your stakeholders as a robust solution for real-world**\n",
        "**biomedical data**\n",
        "\n",
        "Ans:-  **A Robust Pipeline for High-Dimensional Biomedical Data üß¨üî¨**\n",
        "\n",
        "Working with high-dimensional biomedical data, such as gene expression profiles, presents a unique challenge: a vast number of features (genes) but a limited number of samples (patients). This \"Curse of Dimensionality\" leads to models that easily overfit, capturing noise instead of a meaningful biological signal.\n",
        "\n",
        "Our proposed solution is a two-stage pipeline that combines **Principal Component Analysis (PCA)** with a **K-Nearest Neighbors (KNN)** classifier. This approach is powerful, robust, and specifically designed for the challenges of this kind of data.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**1. Using PCA to Reduce Dimensionality üå™Ô∏è**\n",
        "\n",
        "To address the high-dimensional nature of the gene expression data, we'll first apply PCA.\n",
        "\n",
        "- Process: We will fit a PCA model on our training data. PCA will identify a new set of orthogonal axes, known as principal components (PCs). These components are linear combinations of the original gene features and are ordered by the amount of variance they explain.\n",
        "\n",
        "- Outcome: By projecting the data onto these principal components, we can compress thousands of gene features into a much smaller number of \"meta-features\" that capture the most significant biological variation in the dataset, effectively filtering out noise and redundancy.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**2. Deciding How Many Components to Keep ü§î**\n",
        "\n",
        "This is a crucial step to avoid information loss. We will use a data-driven approach to select the optimal number of components.\n",
        "\n",
        "- Explained Variance Threshold: We will train a PCA model and examine the cumulative explained variance ratio. The goal is to select the minimum number of principal components that collectively explain a high percentage of the total variance (e.g., 90-95%). This allows us to reduce dimensionality drastically while retaining the vast majority of the dataset's information.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**3. Using KNN for Classification Post-PCA ü§ñ**\n",
        "\n",
        "With our data now in a low-dimensional, information-rich space, it's perfectly suited for a distance-based algorithm like KNN.\n",
        "\n",
        "Process: We will train a KNN classifier on the PCA-transformed training data. When a new patient's gene expression data needs to be classified, it will first be transformed using the same PCA model. Then, KNN will identify the 'k' closest neighbors in this reduced space and assign the new patient's cancer type based on a majority vote among those neighbors.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "4. **Evaluating the Model**  **üìà**\n",
        "To ensure our model's performance is trustworthy, we must evaluate it on unseen data.\n",
        "\n",
        "- Methodology: We will use a train_test_split to partition the dataset. The entire pipeline (PCA and KNN) will be trained on the training set. The final model's performance will be evaluated on the separate, held-out test set using metrics like accuracy, precision, and recall. This guarantees an unbiased measure of the model's ability to generalize to new patients.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**5. Justification to Stakeholders ü§ù**\n",
        "Our pipeline provides a robust and defensible solution for real-world biomedical data.\n",
        "\n",
        "- Robustness: By using PCA, we explicitly address the high-dimensionality problem and mitigate the risk of overfitting, leading to a more reliable model that generalizes well to new patient samples.\n",
        "\n",
        "- Computational Efficiency: The dimensionality reduction makes the model significantly faster to train and use, which is critical for handling large-scale datasets in the future.\n",
        "\n",
        "- Evidence-Based Decision: The number of components is not an arbitrary choice; it is based on a quantifiable measure (explained variance ratio), providing a transparent and justifiable approach. This pipeline provides a reliable, data-driven tool for clinical decision-making.\n",
        "\n",
        "\n",
        "**Python Code and Output**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Pb8mx-WvO5Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Simulate a high-dimensional gene expression dataset\n",
        "# (1000 features, 200 samples)\n",
        "X, y = make_classification(n_samples=200, n_features=1000, n_informative=50, n_redundant=100,\n",
        "                           n_classes=3, n_clusters_per_class=1, random_state=42)\n",
        "print(\"--------------------------------------------------\")\n",
        "print(\"Simulated Dataset Information\")\n",
        "print(\"--------------------------------------------------\")\n",
        "print(f\"Original number of features: {X.shape[1]}\")\n",
        "print(f\"Number of samples: {X.shape[0]}\\n\")\n",
        "\n",
        "# 2. Split and scale the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 3. Use PCA to decide on the number of components\n",
        "pca = PCA(n_components=None) # Retain all components for analysis\n",
        "pca.fit(X_train_scaled)\n",
        "\n",
        "# Find the number of components to retain 95% of variance\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "n_components = np.argmax(cumulative_variance >= 0.95) + 1 # Add 1 because of 0-based index\n",
        "\n",
        "print(\"--------------------------------------------------\")\n",
        "print(\"PCA Analysis\")\n",
        "print(\"--------------------------------------------------\")\n",
        "print(f\"Components needed to explain 95% variance: {n_components}\")\n",
        "print(f\"Total variance explained by {n_components} components: {cumulative_variance[n_components-1]:.2%}\\n\")\n",
        "\n",
        "# 4. Train PCA with the optimal number of components and transform data\n",
        "pca_optimal = PCA(n_components=n_components)\n",
        "X_train_pca = pca_optimal.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca_optimal.transform(X_test_scaled)\n",
        "\n",
        "print(\"--------------------------------------------------\")\n",
        "print(\"Model Training & Evaluation\")\n",
        "print(\"--------------------------------------------------\")\n",
        "\n",
        "# 5. Train and evaluate KNN classifier on the PCA-transformed data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred = knn_pca.predict(X_test_pca)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Number of features after PCA: {X_train_pca.shape[1]}\")\n",
        "print(f\"Final model accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqq59i9kRJXj",
        "outputId": "e450333e-9f02-471d-fdc0-9af7f5cf5dea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "Simulated Dataset Information\n",
            "--------------------------------------------------\n",
            "Original number of features: 1000\n",
            "Number of samples: 200\n",
            "\n",
            "--------------------------------------------------\n",
            "PCA Analysis\n",
            "--------------------------------------------------\n",
            "Components needed to explain 95% variance: 123\n",
            "Total variance explained by 123 components: 95.08%\n",
            "\n",
            "--------------------------------------------------\n",
            "Model Training & Evaluation\n",
            "--------------------------------------------------\n",
            "Number of features after PCA: 123\n",
            "Final model accuracy: 0.5333\n"
          ]
        }
      ]
    }
  ]
}